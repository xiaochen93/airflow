{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522f19d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run \"Libraries.ipynb\"\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.utils import io\n",
    "#with io.capture_output() as captured:\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfad652-10ea-4c81-b2f7-4d5480736353",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Using cached googlesearch-python-1.2.3.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from googlesearch-python) (4.11.2)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from googlesearch-python) (2.28.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from requests>=2.20->googlesearch-python) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liuxi\\anaconda3\\envs\\web_crawler\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.1.1)\n",
      "Building wheels for collected packages: googlesearch-python\n",
      "  Building wheel for googlesearch-python (setup.py): started\n",
      "  Building wheel for googlesearch-python (setup.py): finished with status 'done'\n",
      "  Created wheel for googlesearch-python: filename=googlesearch_python-1.2.3-py3-none-any.whl size=4224 sha256=0133a24ffb9d0599b24b4c0573d84c27ec2410325e1bdd6076aa4ac9360a61c1\n",
      "  Stored in directory: c:\\users\\liuxi\\appdata\\local\\pip\\cache\\wheels\\98\\24\\e9\\6c225502948c629b01cc895f86406819281ef0da385f3eb669\n",
      "Successfully built googlesearch-python\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef1196e-d419-4b5f-b7db-8efb31e0b8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuxi\\OneDrive\\Documents\\airflow_tutorial\\src\\news_comments_crawlers\\selenium_crawlers\n"
     ]
    }
   ],
   "source": [
    "def get_datetime(days=1, hours=1):\n",
    "    from datetime import datetime\n",
    "    today = datetime.now().replace(microsecond=0)\n",
    "    import datetime\n",
    "    one_day = datetime.timedelta(days=days)\n",
    "    one_hour = datetime.timedelta(hours=hours)\n",
    "    last24hours = today - one_day - one_hour\n",
    "    return today, last24hours\n",
    "\n",
    "today, last24hours=get_datetime()\n",
    "\n",
    "url= \"https://old.reddit.com/r/singapore/top/\"\n",
    "\n",
    "INSERT_API = 'http://10.2.56.213:8086/insert'\n",
    "\n",
    "PING_API = 'http://10.2.56.213:8086/ping'\n",
    "\n",
    "source_id = 16\n",
    "\n",
    "driver = selenium_init(headless=False)\n",
    "\n",
    "table = 'dsta_db.test'\n",
    "\n",
    "latest=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f604707-f558-4437-902a-47185a0d20bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver = Service(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0397918f-b9db-4154-bd6f-72a72fdc9943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\liuxi\\\\.wdm\\\\drivers\\\\chromedriver\\\\win32\\\\103.0.5060\\\\chromedriver.exe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acd7f03-c784-4bed-a236-982761a07989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--DEBUG: API is alive. Message: API is alive\n"
     ]
    }
   ],
   "source": [
    "ping_response = requests.get(PING_API)\n",
    "\n",
    "if ping_response.status_code == 200:\n",
    "    data = ping_response.json()\n",
    "    message = data[\"message\"]\n",
    "    print(f\"\\n--DEBUG: API is alive. Message: {message}\")\n",
    "else:\n",
    "    print(\"\\n--DEBUG: Failed to reach the API\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bec038a",
   "metadata": {},
   "source": [
    "# STEP 1 - Gather post url from the Reddit Forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea54902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getRedditPostItems(driver, url,label):\n",
    "    out= []\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    #1. click to the time series\n",
    "    xpath_dropdown = \"//div[contains(@class,'dropdown lightdrop')]\"\n",
    "    xpath_choices = \"//DIV[contains(@class,'drop-choices')]//A[contains(@class,'choice')]\"\n",
    "    #1. allocate the dropdown menu and click the timeseries\n",
    "    timeseries = getDropdownChoices(driver,xpath_dropdown, xpath_choices)\n",
    "    timeseries = [choice for choice in timeseries if choice.text == label]\n",
    "    \n",
    "    if not timeseries == []:\n",
    "        timeseries[0].click()\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        pass\n",
    "    #3. loop and retrieve items on the page.\n",
    "    searching = True\n",
    "    noOfDocs = 0\n",
    "    while searching:\n",
    "        #2. load main page and pick relative items\n",
    "        xpath_main_page_items = \"//div[contains(@class,'thing id-t3')  and .//SPAN/@title='News']\"\n",
    "        \n",
    "        items = getTableItems(driver, xpath_main_page_items)\n",
    "    \n",
    "        for item in items:\n",
    "            # get the post id\n",
    "            try:\n",
    "                post_id = item.get_property(\"id\")\n",
    "            except NoSuchElementException:\n",
    "                print('-- post id not found')\n",
    "                post_id = \"\"\n",
    "            # get the news title\n",
    "            try:\n",
    "                title = item.find_element(\"xpath\",\".//descendant-or-self::A[contains(@class,'title may-blank')]\").text\n",
    "            except NoSuchElementException:\n",
    "                title = \"\"\n",
    "            # get the news URL\n",
    "            try:\n",
    "                title_url = item.find_element(\"xpath\",\".//descendant-or-self::A[contains(@class,'title may-blank')]\").get_property('href')\n",
    "            except NoSuchElementException:\n",
    "                title_url = \"\"\n",
    "        \n",
    "            # get the datetime\n",
    "            try:\n",
    "                datetime  = item.find_element(\"xpath\", \".//descendant-or-self::p[contains(@class, 'tagline')]/time\").get_attribute('datetime')\n",
    "            except NoSuchElementException:\n",
    "                datetime = \"\"\n",
    "        \n",
    "            # get the domain name\n",
    "            try:\n",
    "                domain = item.find_element(\"xpath\", \".//descendant-or-self::SPAN[contains(@class, 'domain')]\").text\n",
    "            except NoSuchElementException:\n",
    "                domain = \"\"\n",
    "\n",
    "            domain = urlparse(title_url).netloc\n",
    "            # get the news score\n",
    "            try:\n",
    "                scores = item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class, 'score unvoted')]\").text\n",
    "            except NoSuchElementException:\n",
    "                scores = \"\"\n",
    "            # get the no of comments\n",
    "            try:\n",
    "                no_of_cmts = item.find_element(\"xpath\", \".//descendant-or-self::li[contains(@class,'first')]/a\").text.split()[0]\n",
    "            except NoSuchElementException:\n",
    "                no_of_cmts = \"0\"\n",
    "            # get the comment URL\n",
    "            try:\n",
    "                cmt_url = item.find_element(\"xpath\", \".//descendant-or-self::li[contains(@class,'first')]/a\").get_property('href')\n",
    "            except NoSuchElementException:\n",
    "                cmt_url = \"\"\n",
    "            # get the article content\n",
    "            content = getNewsContentByArticle(Article, title_url)\n",
    "\n",
    "            if content == \"\" or \"reddit\" in title_url:\n",
    "                out_dict = getNewsContentByGoogle(title)\n",
    "                content = out_dict['content']\n",
    "                domain = out_dict['domain']\n",
    "                title_url = out_dict['url'] #bug fixed for missing url\n",
    "            \n",
    "            # expand all clickable comment section\n",
    "            \n",
    "            one_instance = {\n",
    "                'article_id': post_id,\n",
    "                'title': title,\n",
    "                'org_title': title,\n",
    "                'url':title_url,\n",
    "                #'post_domain':domain,\n",
    "                'published_datetime': datetime,\n",
    "                #'post_score': scores,\n",
    "                #'post_no_of_cmts': no_of_cmts,\n",
    "                'content': content,\n",
    "                'org_content': content,\n",
    "                'translated': 1,\n",
    "                'last_modified': today,\n",
    "                'cmt_url': cmt_url\n",
    "            }\n",
    "            \n",
    "            out.append(one_instance)\n",
    "            \n",
    "            noOfDocs = noOfDocs + 1\n",
    "            print('\\n\\tDEBUG -- {} no of records have collected.'.format(noOfDocs))\n",
    "            \n",
    "        \n",
    "        # click next to go\n",
    "        xpath_next = \"//div[contains(@class, 'nav-buttons')]//a[contains(text(), 'next')]\"\n",
    "        \n",
    "        # Wait for initialize, in seconds\n",
    "        try:\n",
    "            clickToGo(driver, xpath_next)\n",
    "        except:\n",
    "            print('\\n-- DEBUG: The web scraping is completed.')\n",
    "            break\n",
    "        \n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d87dd11-2123-4a9a-8c44-b2b55e2a705c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tDEBUG -- 1 no of records have collected.\n",
      "\n",
      "\tDEBUG -- 2 no of records have collected.\n",
      "\n",
      "\tDEBUG -- 3 no of records have collected.\n",
      "\n",
      "\tDEBUG -- 4 no of records have collected.\n",
      "\n",
      "\tDEBUG -- 5 no of records have collected.\n",
      "\n",
      "\tDEBUG -- 6 no of records have collected.\n",
      "\n",
      "\tDEBUG -- 7 no of records have collected.\n",
      "\n",
      " -- DEBUG: Errors occured when collecting news/posts on reddit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    items = getRedditPostItems(driver, url, \"past 24 hours\")\n",
    "    # filter out news articles that are posted before the execution datetime of yesterday (existing post)\n",
    "    if latest:\n",
    "        o_length = len(items)\n",
    "        items = [item for item in items if pd.to_datetime(item['published_datetime'], format='%Y-%m-%dT%H:%M:%S+00:00') >= last24hours]\n",
    "\n",
    "        print(f'\\n--DEBUG: {len(items)} posts have been made in the past 24 hours, total {o_length}')\n",
    "except:\n",
    "    print('\\n -- DEBUG: Errors occured when collecting news/posts on reddit.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0d75bf-5dc9-407e-9070-583ed0ed3b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--DEBUG: DB successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # insert data into database\n",
    "    for item in items:\n",
    "        data = {'org_title': item['title'],\n",
    "            'title': item['title'],\n",
    "            'org_content': item['content'],\n",
    "            'url': item['url'] + '|' + item['cmt_url'],\n",
    "            'content': item['content'],\n",
    "            'translated':1,\n",
    "            'lang': 'EN',\n",
    "            'source_id': source_id,\n",
    "            'published_datetime': item['published_datetime']\n",
    "           }\n",
    "        try:\n",
    "            response = requests.post(INSERT_API,json={'table':'dsta_db.test', 'data': data })\n",
    "        except:\n",
    "            print(f'\\n--DEBUG: 1 post is failed to be added {str(item[url])}')\n",
    "    print(f'\\n--DEBUG: DB successful.')\n",
    "except Exception as e:\n",
    "    print(f'\\n--DEBUG: Error occured for insertion {e}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebd598fc-1ab5-40ab-ba58-fcd4d1c7c2a1",
   "metadata": {},
   "source": [
    "# STEP 2 - Gather comments from Each Post Scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f8ff163-9d35-4f39-a268-6f97b5a82286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getCommentsList(items,p_id=\"\"):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for idx, comment_item in enumerate(items):\n",
    "        #1. comment id \n",
    "        try:\n",
    "            c_id = (comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class, 'comment')]\").get_attribute(\"id\")).split('_')[-1]\n",
    "        except NoSuchElementException:\n",
    "            c_id = \"\"\n",
    "            \n",
    "        #2. comment user\n",
    "        try:\n",
    "            c_user = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'entry unvoted')]//p[contains(@class, 'tagline')]/a[contains(@class,'author')]\").text\n",
    "        except NoSuchElementException:\n",
    "            c_user = ''\n",
    "            \n",
    "        #3. comment datetime\n",
    "        try:\n",
    "            c_datetime = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'entry unvoted')]//p[contains(@class, 'tagline')]/time\").get_attribute('datetime')\n",
    "        except NoSuchElementException:\n",
    "            c_datetime = ''\n",
    "        \n",
    "        #4. comment text\n",
    "        try:\n",
    "            c_text = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class, 'md')]\").text\n",
    "        except NoSuchElementException:\n",
    "            c_text = ''\n",
    "            \n",
    "        #5. post score\n",
    "        try:\n",
    "            c_score = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'entry unvoted')]//p[contains(@class, 'tagline')]//span[contains(@class,'score unvoted')]\").text\n",
    "        except NoSuchElementException:\n",
    "            c_score = ''\n",
    "        \n",
    "        one_comment_item ={\n",
    "                'cmt_id' : c_id,\n",
    "                'cmt_replyTo' : p_id,\n",
    "                'cmt_user': c_user,\n",
    "                'cmt_likes':c_score,\n",
    "                'cmt_content': c_text,\n",
    "                'cmt_org_content': c_text,\n",
    "                'cmt_published_datetime': c_datetime,\n",
    "                'translated': 1,\n",
    "                'lang': 'EN',\n",
    "                'source_id': 16\n",
    "        }\n",
    "        if c_id != '' and c_user != '' and c_text != '' and c_datetime != '':\n",
    "            out.append(one_comment_item)\n",
    "        #6. check if has child element:\n",
    "        try:\n",
    "            # This comment has a child element, recursion goes on:\n",
    "            child_element = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'child')]\")\n",
    "            this_child = child_element.find_elements(\"xpath\",\"./div[contains(@class,'sitetable listing')]/div[contains(@class,'thing') and not(contains(@class,'morechildren'))]\")\n",
    "            out.extend(getCommentsList(this_child, p_id=c_id))\n",
    "        except Exception as e:\n",
    "            # This comment does not has a child element:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getRedditCommentItems(driver, c_url):\n",
    "    \n",
    "    driver.get(c_url)\n",
    "    \n",
    "    time.sleep(2.5)\n",
    "    try:\n",
    "        expanders = driver.find_elements(\"xpath\", \"//p[contains(@class, 'tag')]/a[text()='[+]']\")\n",
    "        for each in expanders:\n",
    "            each.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        expanders = driver.find_elements(\"xpath\", \"//span[contains(text(),'more comments')]/a[text()='more comments']\")\n",
    "        for each in expanders:\n",
    "            each.click()\n",
    "    except NoSuchElementException:\n",
    "        pass    \n",
    "    \n",
    "    xpath_comments_dir = \"//div[contains(@class, 'sitetable nestedlisting')]/div[contains(@class, 'thing')]\"\n",
    "    \n",
    "    #root_items = driver.find_elements(\"xpath\", xpath_comments_dir)\n",
    "    \n",
    "    root_items = getTableItems(driver, xpath_comments_dir)\n",
    "    \n",
    "    out = getCommentsList(root_items, p_id=\"\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9b61ac4-7539-4854-abdd-9884f600e11f",
   "metadata": {},
   "source": [
    "# Execution - collect news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8473ea3-1e8b-4de9-b9b6-a6fcbc4a1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select news post that are posted in the latest 2 weeks include today\n",
    "try:\n",
    "    QUERY_API = 'http://10.2.56.213:8086/query'\n",
    "    response = requests.post(QUERY_API, json={'query':\"SELECT article_id, url from dsta_db.test where source_id=16;\"})\n",
    "    json_payload = (json.loads(response.text))\n",
    "except:\n",
    "    print('\\n--DEBUG: selection is bug .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72e332c-2509-4a97-bcf6-dc79a5a1e71f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    items = json_payload['result']\n",
    "    for item in items:\n",
    "        article_id = item['article_id']\n",
    "        cmt_url = item['url'].split('|')[-1]\n",
    "        try:\n",
    "            comments = getRedditCommentItems(driver, cmt_url)\n",
    "            item['comments'] = comments\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('\\n', item['url'], e)\n",
    "            \n",
    "except Exception as e:\n",
    "    print('\\n--DEBUG: error with json payload object, no json is fetched', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7118b599-50ef-45bb-aa75-92991b24621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36067cf4-1978-4db6-be08-88baca1fa41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest:\n",
    "    for item in items:\n",
    "        comments = [comment for comment in item['comments'] if pd.to_datetime(comment['cmt_published_datetime'], format='%Y-%m-%dT%H:%M:%S+00:00') >= last24hours]\n",
    "        item['comments'] = comments\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab5e602d-cdb4-4d57-b644-5dc9f438e706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--DEBUG: DB successful.\n"
     ]
    }
   ],
   "source": [
    "# add article and comments into the database \n",
    "try:\n",
    "    # insert data into database\n",
    "    for item in items:\n",
    "        article_id = item['article_id']\n",
    "        comments = item['comments']\n",
    "        for comment in comments:\n",
    "            data = {\n",
    "                    'cmt_article_id': article_id,\n",
    "                    'cmt_id' : comment['cmt_id'],\n",
    "                    'cmt_replyTo' : comment['cmt_replyTo'],\n",
    "                    'cmt_user': comment['cmt_user'],\n",
    "                    'cmt_likes':comment['cmt_likes'],\n",
    "                    'cmt_content': comment['cmt_content'],\n",
    "                    'cmt_org_content': comment['cmt_content'],\n",
    "                    'cmt_published_datetime': comment['cmt_published_datetime'],\n",
    "                    'translated': 1,\n",
    "                    'lang': 'EN',\n",
    "                    'source_id': 16\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.post(INSERT_API,json={'table':'dsta_db.test_24hr_comments', 'data': data })\n",
    "                if response.status_code != 200:\n",
    "                    print(response)\n",
    "                    print(response.text)\n",
    "                    print(data)\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                print(f'\\n--DEBUG: comment insertion failed {e}')\n",
    "        \n",
    "    print(f'\\n--DEBUG: DB successful.')\n",
    "\n",
    "except Exception as e:\n",
    "    print('\\n--DEBUG: Error occured for insertion', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5ee41b2-d9d3-4e1e-bb5e-890b3d9562af",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4f4f0b8-3c3b-4d5b-a2e1-2907b3473bef",
   "metadata": {},
   "source": [
    "# Save the data locally (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73acbacb-e872-45f3-8f3c-1bb52570fb3a",
   "metadata": {},
   "source": [
    "with open('data/reddit-23-.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(items , output_file ,indent = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
