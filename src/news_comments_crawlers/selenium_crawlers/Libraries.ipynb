{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725695e8-c4c8-4aa4-9cfa-d73576021808",
   "metadata": {},
   "source": [
    "# Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9857885e-dd52-4a33-8fb5-aef6b9fb0df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver    \n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import word_tokenize, Text, FreqDist\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "import pathlib\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "# importing itertools for accumulate()\n",
    "import itertools\n",
    "# importing functools for reduce()\n",
    "import functools\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json\n",
    "import praw\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import display, clear_output\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ce674-9573-4797-9ef9-aaaddd9a4aaf",
   "metadata": {},
   "source": [
    "# Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74261841-9749-4a4d-a21f-85ba2a050b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tests_extract_domain = [\n",
    "    'https://www.independent.co.uk/news/singapore-malaysia-kuala-lumpur-facebook-lawyers-b2017147.html',\n",
    "    'https://independent.co.uk/news/singapore-malaysia-kuala-lumpur-facebook-lawyers-b2017147.html'\n",
    "]\n",
    "def extract_domain(url):\n",
    "    #pattern = r'(?<=\\/|\\.)(\\w+)(?=\\.)'\n",
    "    pattern = r'(www\\.|\\..+\\b)'\n",
    "    url = urlparse(url).netloc\n",
    "    domain = re.sub(pattern,'',url)\n",
    "    if domain == 'sg':\n",
    "        domain = 'Yahoo'\n",
    "    return domain\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text) # padding\n",
    "    text = re.sub('\\s{2,}', ' ', text) # padding\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','', text) \n",
    "    word_list = word_tokenize(text)\n",
    "    stem_text = [lemmatizer.lemmatize(text) for text in word_list if not text.isdigit() if not text.lower() in stop_words]\n",
    "    final_text = \" \".join(stem_text)\n",
    "    \n",
    "    return final_text.strip()\n",
    "\n",
    "def drop_na(df):\n",
    "    df = df.applymap(lambda x: np.nan if x==\"\" else x)\n",
    "    na_num = max(df.isnull().sum())\n",
    "    if na_num == 0:\n",
    "        print('\\n-- No records contains null value, passed')  \n",
    "    else:\n",
    "        print('\\n-- {} records have been dropped due to null values'.format(na_num))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.heatmap(df.isna().transpose(),\n",
    "                    cmap=\"YlGnBu\",\n",
    "                    cbar_kws={'label': 'Missing Data'})\n",
    "        #plt.savefig(\"visualizing_missing_data_with_heatmap_Seaborn_Python.png\", dpi=100)     \n",
    "        df.dropna(inplace=True)\n",
    "        df.isnull().sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b981f-40c1-4e8f-b364-efba251b0e45",
   "metadata": {},
   "source": [
    "# Rendering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf059c-7e90-4d09-96ae-4030477d436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wc(title, path, words):\n",
    "    #color_list=['#050505','#393939','#757474']\n",
    "    #colormap=colors.ListedColormap(color_list)\n",
    "    wc = WordCloud(  \n",
    "        background_color='white',       \n",
    "        max_words=100, \n",
    "        colormap='twilight', \n",
    "        max_font_size=1000\n",
    "        #random_state=1\n",
    "        #font_path = 'Times New Roman'\n",
    "    )\n",
    "    #print(words)\n",
    "    wordcloud = wc.generate_from_frequencies(words)\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title,fontsize=15)\n",
    "    #plt.savefig('{}/wc-{}.png'.format(path,title[:12].replace('.','')))\n",
    "    \n",
    "def plot_bars(counts,words,title,path):\n",
    "    font_size = 15\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    bars = ax.bar(counts, words,color=(0.2, 0.4, 0.6, 0.6),align='center', width=0.8)\n",
    "    \n",
    "    ax.set_title(title, fontsize=font_size,pad=50)\n",
    "    ax.title.set_color(\"Black\")\n",
    "    ax.set_ylabel('TF', fontsize=font_size)\n",
    "    \n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    for rect in bars:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "            int(height),\n",
    "        ha='center', va='bottom',fontsize=round(font_size*0.8))\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=font_size*1.2)\n",
    "    ax.tick_params(axis='y', labelsize=font_size*1.2)\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    leg = ax.legend()\n",
    "    plt.show()\n",
    "    fig.set_size_inches((15, 15), forward=False)\n",
    "    #fig.savefig('{}/{}.png'.format(path,title[:12].replace('.','')))\n",
    "    \n",
    "def plotFreqDistByGroup(df, name = 'domain',group_label = None, freq_label= None):\n",
    "    y_values,x_labels = list(), list()\n",
    "    for x_label, y_count in df.groupby(group_label)[freq_label].count().items():\n",
    "        y_values.append(y_count)\n",
    "        x_labels.append(x_label)\n",
    "    plot_bars(x_labels,y_values,'Total no of {} by {} on {}'.format(freq_label,group_label,name),'')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
