{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "725695e8-c4c8-4aa4-9cfa-d73576021808",
   "metadata": {},
   "source": [
    "# Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9857885e-dd52-4a33-8fb5-aef6b9fb0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver    \n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import word_tokenize, Text, FreqDist\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "import pathlib\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "# importing itertools for accumulate()\n",
    "import itertools\n",
    "# importing functools for reduce()\n",
    "import functools\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json\n",
    "import praw\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b74ce674-9573-4797-9ef9-aaaddd9a4aaf",
   "metadata": {},
   "source": [
    "# Functions - Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74261841-9749-4a4d-a21f-85ba2a050b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tests_extract_domain = [\n",
    "    'https://www.independent.co.uk/news/singapore-malaysia-kuala-lumpur-facebook-lawyers-b2017147.html',\n",
    "    'https://independent.co.uk/news/singapore-malaysia-kuala-lumpur-facebook-lawyers-b2017147.html'\n",
    "]\n",
    "def extract_domain(url):\n",
    "    #pattern = r'(?<=\\/|\\.)(\\w+)(?=\\.)'\n",
    "    pattern = r'(www\\.|\\..+\\b)'\n",
    "    url = urlparse(url).netloc\n",
    "    domain = re.sub(pattern,'',url)\n",
    "    if domain == 'sg':\n",
    "        domain = 'Yahoo'\n",
    "    return domain\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text) # padding\n",
    "    text = re.sub('\\s{2,}', ' ', text) # padding\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','', text) \n",
    "    word_list = word_tokenize(text)\n",
    "    stem_text = [lemmatizer.lemmatize(text) for text in word_list if not text.isdigit() if not text.lower() in stop_words]\n",
    "    final_text = \" \".join(stem_text)\n",
    "    \n",
    "    return final_text.strip()\n",
    "\n",
    "def drop_na(df):\n",
    "    df = df.applymap(lambda x: np.nan if x==\"\" else x)\n",
    "    na_num = max(df.isnull().sum())\n",
    "    if na_num == 0:\n",
    "        print('\\n-- No records contains null value, passed')  \n",
    "    else:\n",
    "        print('\\n-- {} records have been dropped due to null values'.format(na_num))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        sns.heatmap(df.isna().transpose(),\n",
    "                    cmap=\"YlGnBu\",\n",
    "                    cbar_kws={'label': 'Missing Data'})\n",
    "        #plt.savefig(\"visualizing_missing_data_with_heatmap_Seaborn_Python.png\", dpi=100)     \n",
    "        df.dropna(inplace=True)\n",
    "        df.isnull().sum()\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "074b981f-40c1-4e8f-b364-efba251b0e45",
   "metadata": {},
   "source": [
    "# Functions - Rendering & Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2cf059c-7e90-4d09-96ae-4030477d436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wc(title, path, words):\n",
    "    #color_list=['#050505','#393939','#757474']\n",
    "    #colormap=colors.ListedColormap(color_list)\n",
    "    wc = WordCloud(  \n",
    "        background_color='white',       \n",
    "        max_words=100, \n",
    "        colormap='twilight', \n",
    "        max_font_size=1000\n",
    "        #random_state=1\n",
    "        #font_path = 'Times New Roman'\n",
    "    )\n",
    "    #print(words)\n",
    "    wordcloud = wc.generate_from_frequencies(words)\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title,fontsize=15)\n",
    "    #plt.savefig('{}/wc-{}.png'.format(path,title[:12].replace('.','')))\n",
    "    \n",
    "def plot_bars(counts,words,title,path):\n",
    "    font_size = 15\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    bars = ax.bar(counts, words,color=(0.2, 0.4, 0.6, 0.6),align='center', width=0.8)\n",
    "    \n",
    "    ax.set_title(title, fontsize=font_size,pad=50)\n",
    "    ax.title.set_color(\"Black\")\n",
    "    ax.set_ylabel('TF', fontsize=font_size)\n",
    "    \n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    for rect in bars:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "            int(height),\n",
    "        ha='center', va='bottom',fontsize=round(font_size*0.8))\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=font_size*1.2)\n",
    "    ax.tick_params(axis='y', labelsize=font_size*1.2)\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    leg = ax.legend()\n",
    "    plt.show()\n",
    "    fig.set_size_inches((15, 15), forward=False)\n",
    "    #fig.savefig('{}/{}.png'.format(path,title[:12].replace('.','')))\n",
    "    \n",
    "def plotFreqDistByGroup(df, name = 'domain',group_label = None, freq_label= None):\n",
    "    y_values,x_labels = list(), list()\n",
    "    for x_label, y_count in df.groupby(group_label)[freq_label].count().items():\n",
    "        y_values.append(y_count)\n",
    "        x_labels.append(x_label)\n",
    "    plot_bars(x_labels,y_values,'Total no of {} by {} on {}'.format(freq_label,group_label,name),'')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d8b5c42-0fe3-4ad8-9034-2aa2dcdacaa3",
   "metadata": {},
   "source": [
    "# Selenium Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dc25a7f-20b0-4749-b1ac-dbc54aeaff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Input : Xpath for the dropdown list \n",
    "Output: Xpath for the choices in the dropdown\n",
    "\n",
    "'''\n",
    "\n",
    "def getNewsContentByArticle(Article, url):\n",
    "    # get content \n",
    "    try:\n",
    "        content = Article(url)\n",
    "        content.download()\n",
    "        content.parse()\n",
    "        content = content.text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        content = \"\"\n",
    "    return content\n",
    "\n",
    "def getNewsContentByGoogle(title):\n",
    "    \n",
    "    from googlesearch import search\n",
    "\n",
    "    query = \"news:\" + title\n",
    " \n",
    "    content, domain, url = \"\", \"\", \"\"\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "        for j in search(query):\n",
    "            url = j\n",
    "\n",
    "            domain = urlparse(j).netloc\n",
    "            \n",
    "            content = getNewsContentByArticle(Article,j)\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "            if not content != \"\" or count > 10:\n",
    "                break    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {'content':content, 'domain':domain,'url':url}\n",
    "        \n",
    "\n",
    "def getPostListings(driver, xpath_items):\n",
    "    try:\n",
    "        items = driver.find_elements(\"xpath\", xpath_items)\n",
    "    except Excpetion as e:\n",
    "        print('\\n Empty post listing due to ',e)\n",
    "        items = []\n",
    "        pass\n",
    "    return items\n",
    "\n",
    "def getTableItems(driver, xpath_items):\n",
    "    try:\n",
    "        items = driver.find_elements(\"xpath\", xpath_items)\n",
    "    except Exception as e:\n",
    "        items = []\n",
    "        print(e)\n",
    "        pass\n",
    "    return items\n",
    "\n",
    "def getDropdownChoices(driver, xpath_dropdown, xpath_choices):\n",
    "    # Wait for initialize, in seconds\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    dropdown = wait.until(EC.visibility_of_element_located((By.XPATH, xpath_dropdown)))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    dropdown.click()\n",
    "    \n",
    "    #2. find the choices on the list\n",
    "    try:\n",
    "        dropdown = driver.find_elements(\"xpath\",xpath_choices)\n",
    "    except ElementClickInterceptedException:\n",
    "        print('\\n-- No dropdown list found --')\n",
    "        pass\n",
    "    \n",
    "    return dropdown\n",
    "\n",
    "def getChildElement(node, xpath):\n",
    "    xpath = \".//descendant-or-self::\" + xpath\n",
    "    \n",
    "    try:\n",
    "        child_node = node.find_element(\"xpath\", xpath)\n",
    "    except NoSuchElementException:\n",
    "        print(\"\\n-- Unable to find the child element\")\n",
    "        raise\n",
    "        \n",
    "    return child_node\n",
    "\n",
    "\n",
    "def clickToGo(driver, xpath):\n",
    "    try:\n",
    "        button = driver.find_element(\"xpath\",xpath)\n",
    "        button.click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        #print('\\n-- No element {} found --'.format(xpath))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "435345aa-43d7-46c7-9408-a571f34ec9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selenium_init(headless=True):\n",
    "    #initalise crawler option(s)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    options.add_argument('--profile-directory=Default') \n",
    "    ROOT_DIR = pathlib.Path().absolute()\n",
    "    print(ROOT_DIR)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    #driver = webdriver.Chrome(executable_path=(str(ROOT_DIR)+\"/chromedriver\"), options=options)\n",
    "    driver.set_window_size(800, 600)\n",
    "    return driver\n",
    "\n",
    "def getWebElementAttribute(item, xpath, name):\n",
    "    try:\n",
    "        text = item.find_element(\"xpath\", xpath).get_property(name)\n",
    "    except Exception as e:\n",
    "        print('\\n--',e)\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "def getWebElementText(item, xpath):\n",
    "    try:\n",
    "        text = item.find_element(\"xpath\", xpath).text\n",
    "    except Exception as e:\n",
    "        print('\\n--',e)\n",
    "        text = ''\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d52dfdc9-b333-4ca4-a5bc-1332bdf25096",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dff55fa-1bca-42bf-9a71-1385e5f275de",
   "metadata": {},
   "source": [
    "## Save pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12bab505-8e61-4e9a-9cbc-e558db3407e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f662bf0-c220-4a57-acf8-f57388bb448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(data):\n",
    "    # Write data to the CSV file one row at a time\n",
    "    writer = csv.writer(open(\"data.csv\", \"w\"))\n",
    "    for row in data:\n",
    "        writer.writerow(str(row))\n",
    "        yield ','.join(str(row)) + '\\n' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2da84c0b-9172-430f-8b61-24b58e8ef7f8",
   "metadata": {},
   "source": [
    "## load pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "644fdeb2-6b9f-4a59-84a1-ef2cb7b8fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_object(file_path):\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        data = pickle.load(input_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26857457-4ae6-4de2-b1af-adf38c5d573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getNewsContentByGoogle('Thai Election is not right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8a0eb-8388-48cf-beec-9e40d7a1c599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
