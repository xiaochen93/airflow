{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "522f19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver    \n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import word_tokenize, Text, FreqDist\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "import pathlib\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "# importing itertools for accumulate()\n",
    "import itertools\n",
    "# importing functools for reduce()\n",
    "import functools\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json\n",
    "import praw\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import display, clear_output\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec038a",
   "metadata": {},
   "source": [
    "# Selenium Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5657cda6-92e0-48a6-ba50-a8fa5a344bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Input : Xpath for the dropdown list \n",
    "Output: Xpath for the choices in the dropdown\n",
    "\n",
    "'''\n",
    "def getNewsContentByArticle(Article, url):\n",
    "    # get content \n",
    "    try:\n",
    "        content = Article(url)\n",
    "        content.download()\n",
    "        content.parse()\n",
    "        content = content.text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        content = \"\"\n",
    "    return content\n",
    "\n",
    "def getNewsContentByGoogle(title):\n",
    "    \n",
    "    try:\n",
    "        from googlesearch import search\n",
    "    except ImportError:\n",
    "        print(\"No module named 'google' found\")\n",
    "\n",
    "    # to search\n",
    "    query = \"news:\" + title\n",
    " \n",
    "    content, domain, url = \"\", \"\", \"\"\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        for j in search(query):\n",
    "            url = j\n",
    "\n",
    "            domain = urlparse(j).netloc\n",
    "\n",
    "            isIn = [True for c in channels if c in domain]\n",
    "\n",
    "            content = getNewsContentByArticle(Article,j)\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "            if not isIn == [] or count > 10:\n",
    "                break\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {'content':content, 'domain':domain,'url':url}\n",
    "        \n",
    "\n",
    "def getTableItems(driver, xpath_items):\n",
    "    try:\n",
    "        items = driver.find_elements(\"xpath\", xpath_items)\n",
    "    except Exception as e:\n",
    "        items = []\n",
    "        print(e)\n",
    "        pass\n",
    "    return items\n",
    "\n",
    "def getDropdownChoices(driver, xpath_dropdown, xpath_choices):\n",
    "    # Wait for initialize, in seconds\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    dropdown = wait.until(EC.visibility_of_element_located((By.XPATH, xpath_dropdown)))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    dropdown.click()\n",
    "    \n",
    "    #2. find the choices on the list\n",
    "    try:\n",
    "        dropdown = driver.find_elements(\"xpath\",xpath_choices)\n",
    "    except ElementClickInterceptedException:\n",
    "        print('\\n-- No dropdown list found --')\n",
    "        pass\n",
    "    \n",
    "    return dropdown\n",
    "\n",
    "def getChildElement(node, xpath):\n",
    "    \n",
    "    xpath = \".//descendant-or-self::\" + xpath\n",
    "    \n",
    "    try:\n",
    "        child_node = node.find_element(\"xpath\", xpath)\n",
    "    except NoSuchElementException:\n",
    "        print(\"\\n-- Unable to find the child element\")\n",
    "        raise\n",
    "        \n",
    "    return child_node\n",
    "\n",
    "\n",
    "def clickToGo(driver, xpath):\n",
    "    try:\n",
    "        button = driver.find_element(\"xpath\",xpath)\n",
    "        button.click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        #print('\\n-- No element {} found --'.format(xpath))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e278786-8afc-4177-a959-647c14f583a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selenium_init():\n",
    "    #initalise crawler option(s)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    options.add_argument('headless')\n",
    "    ROOT_DIR = pathlib.Path().absolute()\n",
    "    print(ROOT_DIR)\n",
    "    driver = webdriver.Chrome(executable_path=(str(ROOT_DIR)+\"/chromedriver\"), options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6902e0-29b8-4d59-97b8-ed09238f55ca",
   "metadata": {},
   "source": [
    "## Getting Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea54902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRedditPostItems(driver, url,label):\n",
    "    out= []\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #1. click to the time series\n",
    "    xpath_dropdown = \"//div[contains(@class,'dropdown lightdrop')]\"\n",
    "    xpath_choices = \"//DIV[contains(@class,'drop-choices')]//A[contains(@class,'choice')]\"\n",
    "    #1. allocate the dropdown menu and click the timeseries\n",
    "    timeseries = getDropdownChoices(driver,xpath_dropdown, xpath_choices)\n",
    "    timeseries = [choice for choice in timeseries if choice.text == label]\n",
    "    \n",
    "    if not timeseries == []:\n",
    "        timeseries[0].click()\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #3. loop and retrieve items on the page.\n",
    "    searching = True\n",
    "    noOfDocs = 0\n",
    "    while searching:\n",
    "        #2. load main page and pick relative items\n",
    "        xpath_main_page_items = \"//div[contains(@class,'thing id-t3')  and .//SPAN/@title='News']\"\n",
    "        \n",
    "        items = getTableItems(driver, xpath_main_page_items)\n",
    "    \n",
    "        for item in items:\n",
    "            # get the post id\n",
    "            try:\n",
    "                post_id = item.get_property(\"id\")\n",
    "            except NoSuchElementException:\n",
    "                print('-- post id not found')\n",
    "                post_id = \"\"\n",
    "            # get the news title\n",
    "            try:\n",
    "                title = item.find_element(\"xpath\",\".//descendant-or-self::A[contains(@class,'title may-blank')]\").text\n",
    "            except NoSuchElementException:\n",
    "                title = \"\"\n",
    "            # get the news URL\n",
    "            try:\n",
    "                title_url = item.find_element(\"xpath\",\".//descendant-or-self::A[contains(@class,'title may-blank')]\").get_property('href')\n",
    "            except NoSuchElementException:\n",
    "                title_url = \"\"\n",
    "            # get the datetime\n",
    "            try:\n",
    "                datetime  = item.find_element(\"xpath\", \".//descendant-or-self::p[contains(@class, 'tagline')]/time\").get_attribute('datetime')\n",
    "            except NoSuchElementException:\n",
    "                datetime = \"\"\n",
    "            # get the domain name\n",
    "            try:\n",
    "                domain = item.find_element(\"xpath\", \".//descendant-or-self::SPAN[contains(@class, 'domain')]\").text\n",
    "            except NoSuchElementException:\n",
    "                domain = \"\"\n",
    "\n",
    "            domain = urlparse(title_url).netloc\n",
    "            # get the news score\n",
    "            try:\n",
    "                scores = item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class, 'score unvoted')]\").text\n",
    "            except NoSuchElementException:\n",
    "                scores = \"\"\n",
    "            # get the no of comments\n",
    "            try:\n",
    "                no_of_cmts = item.find_element(\"xpath\", \".//descendant-or-self::li[contains(@class,'first')]/a\").text.split()[0]\n",
    "            except NoSuchElementException:\n",
    "                no_of_cmts = \"0\"\n",
    "            # get the comment URL\n",
    "            try:\n",
    "                cmt_url = item.find_element(\"xpath\", \".//descendant-or-self::li[contains(@class,'first')]/a\").get_property('href')\n",
    "            except NoSuchElementException:\n",
    "                cmt_url = \"\"\n",
    "            # get the article content\n",
    "            content = getNewsContentByArticle(Article, title_url)\n",
    "\n",
    "            if content == \"\" or \"reddit\" in title_url:\n",
    "                out_dict = getNewsContentByGoogle(title)\n",
    "                content = out_dict['content']\n",
    "                domain = out_dict['domain']\n",
    "                title_url = out_dict['url'] #bug fixed for missing url\n",
    "            \n",
    "            # expand all clickable comment section\n",
    "            \n",
    "            one_instance = {\n",
    "                'post_id': post_id,\n",
    "                'post_title': title,\n",
    "                'post_url':title_url,\n",
    "                'post_domain':domain,\n",
    "                'post_datetime': datetime,\n",
    "                'post_score': scores,\n",
    "                'post_no_of_cmts': no_of_cmts,\n",
    "                'post_article': content,\n",
    "                'cmt_url': cmt_url\n",
    "            }\n",
    "            \n",
    "            out.append(one_instance)\n",
    "            \n",
    "            noOfDocs = noOfDocs + 1\n",
    "            print('\\n-- {} no of records have collected.'.format(noOfDocs))\n",
    "            \n",
    "        \n",
    "        # click next to go\n",
    "        xpath_next = \"//div[contains(@class, 'nav-buttons')]//a[contains(text(), 'next')]\"\n",
    "        \n",
    "        # Wait for initialize, in seconds\n",
    "        try:\n",
    "            clickToGo(driver, xpath_next)\n",
    "        except:\n",
    "            print('\\n-- DEBUG: The web scraping is completed.')\n",
    "            break\n",
    "        \n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd598fc-1ab5-40ab-ba58-fcd4d1c7c2a1",
   "metadata": {},
   "source": [
    "## Getting Comments for Each Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8ff163-9d35-4f39-a268-6f97b5a82286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getCommentsList(items,p_id=\"\"):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for idx, comment_item in enumerate(items):\n",
    "        #1. comment id \n",
    "        try:\n",
    "            c_id = (comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class, 'comment')]\").get_attribute(\"id\")).split('_')[-1]\n",
    "        except NoSuchElementException:\n",
    "            c_id = \"\"\n",
    "            \n",
    "        #2. comment user\n",
    "        try:\n",
    "            c_user = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'entry unvoted')]//p[contains(@class, 'tagline')]/a[contains(@class,'author')]\").text\n",
    "        except NoSuchElementException:\n",
    "            c_user = ''\n",
    "            \n",
    "        #3. comment datetime\n",
    "        try:\n",
    "            c_datetime = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'entry unvoted')]//p[contains(@class, 'tagline')]/time\").get_attribute('datetime')\n",
    "        except NoSuchElementException:\n",
    "            c_datetime = ''\n",
    "        \n",
    "        #4. comment text\n",
    "        try:\n",
    "            c_text = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class, 'md')]\").text\n",
    "        except NoSuchElementException:\n",
    "            c_text = ''\n",
    "            \n",
    "        #5. post score\n",
    "        try:\n",
    "            c_score = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'entry unvoted')]//p[contains(@class, 'tagline')]//span[contains(@class,'score unvoted')]\").text\n",
    "        except NoSuchElementException:\n",
    "            c_score = ''\n",
    "        \n",
    "        one_comment_item ={\n",
    "                'cmt_id' : c_id,\n",
    "                'replyTo' : p_id,\n",
    "                'cmt_user': c_user,\n",
    "                'cmt_score':c_score,\n",
    "                'cmt_text': c_text,\n",
    "                'cmt_datetime': c_datetime\n",
    "        }\n",
    "        if c_id != '' and c_user != '' and c_text != '' and c_datetime != '':\n",
    "            out.append(one_comment_item)\n",
    "        #6. check if has child element:\n",
    "        try:\n",
    "            # This comment has a child element, recursion goes on:\n",
    "            child_element = comment_item.find_element(\"xpath\", \".//descendant-or-self::div[contains(@class,'child')]\")\n",
    "            this_child = child_element.find_elements(\"xpath\",\"./div[contains(@class,'sitetable listing')]/div[contains(@class,'thing') and not(contains(@class,'morechildren'))]\")\n",
    "            out.extend(getCommentsList(this_child, p_id=c_id))\n",
    "        except Exception as e:\n",
    "            # This comment does not has a child element:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getRedditCommentItems(driver, c_url):\n",
    "    driver.get(c_url)\n",
    "    \n",
    "    time.sleep(2.5)\n",
    "    try:\n",
    "        expanders = driver.find_elements(\"xpath\", \"//p[contains(@class, 'tag')]/a[text()='[+]']\")\n",
    "        for each in expanders:\n",
    "            each.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        expanders = driver.find_elements(\"xpath\", \"//span[contains(text(),'more comments')]/a[text()='more comments']\")\n",
    "        for each in expanders:\n",
    "            each.click()\n",
    "    except NoSuchElementException:\n",
    "        pass    \n",
    "    \n",
    "    xpath_comments_dir = \"//div[contains(@class, 'sitetable nestedlisting')]/div[contains(@class, 'thing')]\"\n",
    "    \n",
    "    #root_items = driver.find_elements(\"xpath\", xpath_comments_dir)\n",
    "    \n",
    "    root_items = getTableItems(driver, xpath_comments_dir)\n",
    "    \n",
    "    out = getCommentsList(root_items, p_id=\"\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b61ac4-7539-4854-abdd-9884f600e11f",
   "metadata": {},
   "source": [
    "# Get New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccc759-f15d-467c-9f4c-78ca48337cfb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "url= \"https://old.reddit.com/r/singapore/top/\"\n",
    "driver = selenium_init()\n",
    "try:\n",
    "    items = getRedditPostItems(driver, url,\"past week\")\n",
    "except:\n",
    "    print('\\n -- DEBUG: Errors occured when collecting news/posts on reddit.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0a8bd-c824-45ad-836d-e351a622c32f",
   "metadata": {
    "tags": []
   },
   "source": [
    "for item in items:\n",
    "    try:\n",
    "        item['comments'] = getRedditCommentItems(driver, item['c_url'])\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print('\\n', item['c_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31ab59-3640-4f66-bc87-8bcc9a5e1fd5",
   "metadata": {},
   "source": [
    "with open('data/reddit-12.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(items , output_file ,indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c85c5b-774f-4cb5-b22e-1245f81234f5",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "818cdb14-5ff6-4041-be52-df9baf0a9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def list_directory(path):\n",
    "    onlyfiles = [path+f for f in listdir(path) if f.split('.')[-1] == 'json'] \n",
    "    print('\\nINFO -- File names:',onlyfiles)\n",
    "    return onlyfiles\n",
    "\n",
    "def load_reddit_data(paths):\n",
    "    dic = {}\n",
    "    for path in paths:\n",
    "        try:\n",
    "            new = json.load(open(path,encoding=\"utf8\"))\n",
    "        except Exception as e:\n",
    "            print('\\n-- INFO : ',e, path)\n",
    "        \n",
    "        try:\n",
    "            dic['data'].extend(new['data'])\n",
    "        except Exception as e:\n",
    "            print('\\n-- INFO : ',e, path)\n",
    "            dic = new\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7d0c4d-5d39-49e0-acf0-b9733a1f7763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO -- File names: ['data/reddit/reddit-02.json', 'data/reddit/reddit-03.json', 'data/reddit/reddit-04.json', 'data/reddit/reddit-05.json', 'data/reddit/reddit-06.json', 'data/reddit/reddit-07.json', 'data/reddit/reddit-08.json', 'data/reddit/reddit-09.json', 'data/reddit/reddit-10.json', 'data/reddit/reddit-11.json', 'data/reddit/reddit-12.json']\n",
      "\n",
      "-- INFO :  'data' data/reddit/reddit-02.json\n"
     ]
    }
   ],
   "source": [
    "reddit_data = load_reddit_data(list_directory('data/reddit/'))\n",
    "\n",
    "reddit_df = pd.DataFrame.from_dict(reddit_data['data'],orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c78d5e-ccee-41a1-98d2-dc9fb13fcbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liux5\\Documents\\Project 3 - DSTA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liux5\\AppData\\Local\\Temp\\ipykernel_20788\\1316614122.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=(str(ROOT_DIR)+\"/chromedriver\"), options=options)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00797724723815918,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4854,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a588cc61654a30a6e49e16fbb77208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import concurrent.futures\n",
    "import gc\n",
    "\n",
    "url= \"https://old.reddit.com/r/singapore/top/\"\n",
    "\n",
    "driver = selenium_init()\n",
    "\n",
    "new_comments = {}\n",
    "\n",
    "#driver.get(\"https://old.reddit.com/r/singapore/comments/sp1ex7/privileges_committee_recommends_s35000_fine_for/\")\n",
    "\n",
    "for idx, row in tqdm(reddit_df.iterrows(), total=reddit_df.shape[0]):\n",
    "    c_url = row['c_url']\n",
    "    try:\n",
    "        new_item_cmts = getRedditCommentItems(driver, str(c_url))\n",
    "    except Exception as e:\n",
    "        print('\\n-- INFO:', e, new_item_cmts)\n",
    "        new_item_cmts = []\n",
    "    \n",
    "    new_comments[idx] = new_item_cmts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50381fe3-3657-474a-bbf3-1c6f03d508cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_comments.json\", \"w\") as outfile:\n",
    "    json.dump(new_comments, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2b29b73-7992-4dec-a235-6ec5ee3ae4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_comments_df = {}\n",
    "\n",
    "for idx, comments in new_comments.items():\n",
    "    comments_df = pd.DataFrame.from_records(comments)\n",
    "    all_comments_df[idx] = comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd3aa5a2-8662-4c61-801c-3daba0456ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "save_pickle_object(reddit_df,'news_crawler/output/df/new/REDDIT_EN_1-12.pikle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "019a5cac-ce1e-4497-b2fb-3e365310d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_object(all_comments_df,'comments_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47f18349-1d72-4b05-87f5-ca007a9b0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_object(reddit_df,'news_crawler/output/df/new/REDDIT_EN_1-12.pikle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a3bd72d-4d2b-470a-8542-203d1b6a9068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cmt_id</th>\n",
       "      <th>replyTo</th>\n",
       "      <th>cmt_user</th>\n",
       "      <th>cmt_score</th>\n",
       "      <th>cmt_text</th>\n",
       "      <th>cmt_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hwcguvl</td>\n",
       "      <td></td>\n",
       "      <td>3ply</td>\n",
       "      <td>527 points</td>\n",
       "      <td>\"The committee did not recommend any action ag...</td>\n",
       "      <td>2022-02-10T09:02:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hwci3ta</td>\n",
       "      <td>hwcguvl</td>\n",
       "      <td>ShadeX8</td>\n",
       "      <td>167 points</td>\n",
       "      <td>I’m running out of popcorn. How many more twis...</td>\n",
       "      <td>2022-02-10T09:19:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hwcsrjg</td>\n",
       "      <td>hwci3ta</td>\n",
       "      <td>ShadeX8</td>\n",
       "      <td>102 points</td>\n",
       "      <td>In before it was all a plot from Jamus to take...</td>\n",
       "      <td>2022-02-10T11:39:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hwcxoxq</td>\n",
       "      <td>hwcsrjg</td>\n",
       "      <td>twst222</td>\n",
       "      <td>63 points</td>\n",
       "      <td>He has his own Umbrella Academy</td>\n",
       "      <td>2022-02-10T12:32:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hwdc1s8</td>\n",
       "      <td>hwcxoxq</td>\n",
       "      <td>jermso</td>\n",
       "      <td>16 points</td>\n",
       "      <td>ella</td>\n",
       "      <td>2022-02-10T14:30:01+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hwcdvxp</td>\n",
       "      <td></td>\n",
       "      <td>dittotan</td>\n",
       "      <td>29 points</td>\n",
       "      <td>PAP is really piss-poor at reading the room wh...</td>\n",
       "      <td>2022-02-10T08:23:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>hwce0ry</td>\n",
       "      <td>hwcdvxp</td>\n",
       "      <td>fateoftheg0dz</td>\n",
       "      <td>81 points</td>\n",
       "      <td>Curious, what does the \"general public\" actual...</td>\n",
       "      <td>2022-02-10T08:24:52+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>hwch6eh</td>\n",
       "      <td>hwce0ry</td>\n",
       "      <td>Reclusivechair</td>\n",
       "      <td>63 points</td>\n",
       "      <td>Good qn. I suspect that the \"general public\" m...</td>\n",
       "      <td>2022-02-10T09:07:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>hwcuppr</td>\n",
       "      <td>hwcdvxp</td>\n",
       "      <td>eddalton</td>\n",
       "      <td>9 points</td>\n",
       "      <td>Ironically this comment itself is piss-poor at...</td>\n",
       "      <td>2022-02-10T12:01:13+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>hwchjbk</td>\n",
       "      <td>hwcdvxp</td>\n",
       "      <td>Eskipony</td>\n",
       "      <td>17 points</td>\n",
       "      <td>The government is incapable of doing more than...</td>\n",
       "      <td>2022-02-10T09:12:04+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cmt_id  replyTo        cmt_user   cmt_score  \\\n",
       "0    hwcguvl                     3ply  527 points   \n",
       "1    hwci3ta  hwcguvl         ShadeX8  167 points   \n",
       "2    hwcsrjg  hwci3ta         ShadeX8  102 points   \n",
       "3    hwcxoxq  hwcsrjg         twst222   63 points   \n",
       "4    hwdc1s8  hwcxoxq          jermso   16 points   \n",
       "..       ...      ...             ...         ...   \n",
       "186  hwcdvxp                 dittotan   29 points   \n",
       "187  hwce0ry  hwcdvxp   fateoftheg0dz   81 points   \n",
       "188  hwch6eh  hwce0ry  Reclusivechair   63 points   \n",
       "189  hwcuppr  hwcdvxp        eddalton    9 points   \n",
       "190  hwchjbk  hwcdvxp        Eskipony   17 points   \n",
       "\n",
       "                                              cmt_text  \\\n",
       "0    \"The committee did not recommend any action ag...   \n",
       "1    I’m running out of popcorn. How many more twis...   \n",
       "2    In before it was all a plot from Jamus to take...   \n",
       "3                      He has his own Umbrella Academy   \n",
       "4                                                 ella   \n",
       "..                                                 ...   \n",
       "186  PAP is really piss-poor at reading the room wh...   \n",
       "187  Curious, what does the \"general public\" actual...   \n",
       "188  Good qn. I suspect that the \"general public\" m...   \n",
       "189  Ironically this comment itself is piss-poor at...   \n",
       "190  The government is incapable of doing more than...   \n",
       "\n",
       "                  cmt_datetime  \n",
       "0    2022-02-10T09:02:43+00:00  \n",
       "1    2022-02-10T09:19:30+00:00  \n",
       "2    2022-02-10T11:39:10+00:00  \n",
       "3    2022-02-10T12:32:10+00:00  \n",
       "4    2022-02-10T14:30:01+00:00  \n",
       "..                         ...  \n",
       "186  2022-02-10T08:23:10+00:00  \n",
       "187  2022-02-10T08:24:52+00:00  \n",
       "188  2022-02-10T09:07:04+00:00  \n",
       "189  2022-02-10T12:01:13+00:00  \n",
       "190  2022-02-10T09:12:04+00:00  \n",
       "\n",
       "[191 rows x 6 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a35cb-b481-4153-b954-50e6c2e04193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
